{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, primarily we will focus on data processing techniques in python. Pre-processing refers to the transformations applied to our data before feeding it to the algorithm.Data preprocessing is the first (and arguably most important) step toward building a working machine learning model. It's critical. If you data hasn't been cleaned and preprocessed, you model may not work. It's that simple. Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis. \n",
    "\n",
    "As already discussed, learning algorithms have affinity towards certain data types on which they perform incredibly well. They are also known to give reckless predictions with unscaled or unstandardized features. Machine learning tools are as good as the quality of data. Sophisticated algorithms will not make up for poor data. Data needs to go through a few before it is ready for further use.\n",
    "\n",
    "Pre-processing refers to the transformations applied to data before feeding to the algorithm. In python, Scikit-learn library has a pre-built functionality under sklearn.preprocessing. \n",
    "\n",
    "#### Preprocessing Data\n",
    "\n",
    "*Sklearn* its **preprocessing library**  forms a solid foundation to guide you through this important task in the data science pipeline. It includes all utility functions and transformer classes available in sklearn, supplemented with some useful functions from other common libraries. \n",
    " Th enotebook is structured in a **logical order** representing the order in which one should execute the transfromatins discussed. The following issues will be handled:\n",
    "\n",
    "- Missing values\n",
    "\n",
    "- Outlier detection\n",
    "\n",
    "- Feature scaling\n",
    "\n",
    "- Normalization\n",
    "\n",
    "- Categorical features\n",
    "\n",
    "- Numerical features\n",
    "\n",
    "- Custom transformations\n",
    "\n",
    "- Polynomial features  ( not discussed) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.588928Z",
     "start_time": "2020-06-29T22:03:13.581915Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Dataset\n",
    "\n",
    "Lot of datasets come i CSV formats which can be read using a method called read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.795300Z",
     "start_time": "2020-06-29T22:03:13.593098Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/home/Desktop/Univ/CSTU/PYTHON_NOTEBOOKS/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.806357Z",
     "start_time": "2020-06-29T22:03:13.798910Z"
    },
    "format": "column"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.818421Z",
     "start_time": "2020-06-29T22:03:13.810373Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.834847Z",
     "start_time": "2020-06-29T22:03:13.821812Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.913364Z",
     "start_time": "2020-06-29T22:03:13.839918Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting our dataset carefully, we are going to create a matrix of features in our dataset (X) and create a dependent vector (Y) with their respective observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our dataset, but we need to create a matrix of dependent variables and a vector of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.928627Z",
     "start_time": "2020-06-29T22:03:13.918411Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,2:].values\n",
    "y = df.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing data in Dataset\n",
    "\n",
    "Usually you may find some data are missing. You need to handle the problem when you come across them. Handling missing values is an essential preprocessing task that can drastically deteriorate model when not done with sufficient care. A few questions should come up when handling missing values:\n",
    "\n",
    "*Do I have missing values? How are they expressed in the data? Should I withhold samples with missing values? Or should I replace them? If so, which values should they be replaced with?*\n",
    "\n",
    "Before starting handling missing values it is important to **identify the missing values** and know with which value they are replaced. \n",
    " \n",
    "\n",
    "The library that we are going to use for the task is called **Scikit-learn preprocessing**. It contains a class called **Imputer()** which will help us take care of the missing data.\n",
    "\n",
    "\n",
    "#### how many missing data points are there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.949013Z",
     "start_time": "2020-06-29T22:03:13.932596Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of missing data points in each column\n",
    "missing_values_df = df.isnull().sum()\n",
    "print(\"missing values in each coulmn:\", '\\n')\n",
    "print( missing_values_df)\n",
    "\n",
    "print(\"percentage of missing values in each coulmn:\", '\\n')\n",
    "print( missing_values_df/len(df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be helpful to see what percentage of the values on out dataset were missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:13.961463Z",
     "start_time": "2020-06-29T22:03:13.952895Z"
    }
   },
   "outputs": [],
   "source": [
    "total_points = np.product(df.shape)\n",
    "total_missing_values = missing_values_df.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "(total_missing_values/total_points) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.001141Z",
     "start_time": "2020-06-29T22:03:13.965807Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rows with missing values\n",
    "null_data = df[df.isnull().any(axis=1)]\n",
    "print(\"number of rows containg missing values:\", null_data.shape)\n",
    "\n",
    "null_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out the reason for missing value and take appropriate action as discussed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping missing values\n",
    "If you're sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this. \n",
    "*Syntax* \n",
    "\n",
    "DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.015076Z",
     "start_time": "2020-06-29T22:03:14.004553Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all the rows that contain a missing value\n",
    "df1=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above operation has a problem that it removes every row in the dataset that has atleast one missing value. We might have better luck removing all the coulmns that have at least one missing value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.025659Z",
     "start_time": "2020-06-29T22:03:14.018149Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.041118Z",
     "start_time": "2020-06-29T22:03:14.029752Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if there si any missing value in df1\n",
    "print(\"missing value in new data:\",df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.057446Z",
     "start_time": "2020-06-29T22:03:14.044927Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all columns with atleast one missing value\n",
    "df2 = df.dropna(axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.074000Z",
     "start_time": "2020-06-29T22:03:14.064627Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.093481Z",
     "start_time": "2020-06-29T22:03:14.079375Z"
    }
   },
   "outputs": [],
   "source": [
    "## remove columns which have most of the nonNa's enteries\n",
    "\n",
    "df3 = df.dropna(thresh = 885,axis =1)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputing missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn** provides a *SimpleImputer*. \n",
    "\n",
    "*Syntax*\n",
    "\n",
    "sklearn.impute.SimpleImputer(missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:03:14.289855Z",
     "start_time": "2020-06-29T22:03:14.097659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in missing values for Age\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#imp  = SimpleImputer(missing_values = \"NaN\", strategy = \"mean\")\n",
    "imp  = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "# Age1 = np.array(df.iloc[:,5].values.reshape(-1,1))\n",
    "imp.fit(df[['Age']])\n",
    "df['Age'] = imp.transform(df[['Age']])\n",
    "#df['Age_filled'] =Age_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:00.505021Z",
     "start_time": "2020-06-29T22:04:00.485860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in missing values for Age\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#imp  = SimpleImputer(missing_values = \"NaN\", strategy = \"mean\")\n",
    "imp  = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "Age1 = pd.DataFrame(df.iloc[:,5].values)\n",
    "# Age1 = np.array(df.iloc[:,5].values.reshape(-1,1))\n",
    "imp.fit(Age1)\n",
    "Age_filled = imp.transform(Age1)\n",
    "df['Age_filled'] =Age_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.717679Z",
     "start_time": "2020-06-29T22:04:16.692309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in missing values for Cabin\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#imp  = SimpleImputer(missing_values = \"NaN\", strategy = “most_frequent”)\n",
    "imp  = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\n",
    "imp.fit(df[['Cabin']])\n",
    "df['Cabin'] = imp.transform(df[['Cabin']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.818541Z",
     "start_time": "2020-06-29T22:04:16.721735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in missing values for Embarked\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#imp  = SimpleImputer(missing_values = \"NaN\", strategy = “most_frequent”)\n",
    "imp  = SimpleImputer(missing_values = np.nan, strategy = \"most_frequent\")\n",
    "imp.fit(df[['Embarked']])\n",
    "df['Embarked'] = imp.transform(df[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.830947Z",
     "start_time": "2020-06-29T22:04:16.822687Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.839869Z",
     "start_time": "2020-06-29T22:04:16.835306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the columns with missing values\n",
    "#df_new = df.drop([\"Age\",\"Cabin\"],axis = 1)\n",
    "#df_new = df.drop([\"Age\",\"Cabin\", \"Embarked\"],axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.854073Z",
     "start_time": "2020-06-29T22:04:16.843111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now check if there are any missing values\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provide **fillna()** method to impute missing values. Pandas also provides options to fill forward (ffill) or fill backward (bfill) which are convenient whne working with time series.  Padas provide the other methods also. \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.873299Z",
     "start_time": "2020-06-29T22:04:16.856883Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace all NA's with 0\n",
    "df3 = df.fillna(0)\n",
    "print(\"shape of new data frame:\", df3.shape)\n",
    "\n",
    "print(\"missing values in new data frame:\", df3.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.885676Z",
     "start_time": "2020-06-29T22:04:16.876760Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Age'].fillna(df['Age'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways to detect and remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:16.905037Z",
     "start_time": "2020-06-29T22:04:16.893565Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Draw the box plot from Age and fare as they are continuosu\n",
    "\n",
    "# df_age_fare = [df['Age'], df['Fare']]  \n",
    "\n",
    "# plt.boxplot(df_age_fare)\n",
    "\n",
    "It is very difficult to see clearly as they are on different scale. We will plot them separatel\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:17.152983Z",
     "start_time": "2020-06-29T22:04:16.911384Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df['Age'], whis=0.75)\n",
    "#plt.boxplot(df['Age'], vert=False, whis=0.75)\n",
    "plt.title('Box plot for Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:17.369382Z",
     "start_time": "2020-06-29T22:04:17.156587Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df['Fare'], whis=0.75)\n",
    "plt.title('Box plot for Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:17.703054Z",
     "start_time": "2020-06-29T22:04:17.373044Z"
    }
   },
   "outputs": [],
   "source": [
    "'''with Histogram'''\n",
    "\n",
    "plt.hist(df['Age'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:18.008077Z",
     "start_time": "2020-06-29T22:04:17.706253Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df['Fare'])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discover outliers with Mathematical Function : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Z-Score\n",
    "\n",
    "while calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:18.030088Z",
     "start_time": "2020-06-29T22:04:18.011559Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "df_age_fare = df[['Age', \"Fare\"]]\n",
    "\n",
    "df_age_fare = df_age_fare.dropna()\n",
    "\n",
    "z = np.abs(stats.zscore(df_age_fare))\n",
    "print(z)\n",
    "\n",
    "Threshold = 3\n",
    "\n",
    "print(np.where(z>Threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first array contains the list of row numbers and second array respective column numbers, which mean z[23][1] have a Z-score higher than 3. So the data point - 23rd on column Fare is an outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interquartile range IQR Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:18.089360Z",
     "start_time": "2020-06-29T22:04:18.033602Z"
    }
   },
   "outputs": [],
   "source": [
    "Q1 = df_age_fare.quantile(0.25)\n",
    "Q3 = df_age_fare.quantile(0.75)\n",
    "IQR = Q3-Q1\n",
    "print(\"IQR:\", '\\n')\n",
    "print(IQR)\n",
    "\n",
    "lwr = Q1-1.5*IQR\n",
    "upr = Q3+1.5*IQR\n",
    "\n",
    "'''\n",
    "Get the outliers. \n",
    "\n",
    "'''\n",
    "print((df_age_fare<lwr)|(df_age_fare>upr))\n",
    "\n",
    "\n",
    "##CHECK IT Get the indices.\n",
    "\n",
    "x1 = np.where((df_age_fare<lwr)|(df_age_fare>upr))\n",
    "df_age_fare.iloc[23,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correcting or Removng the outliers\n",
    "\n",
    "Now that we know how to detect the outliers, it is important to understand if they needs to be removed or corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:18.120753Z",
     "start_time": "2020-06-29T22:04:18.093352Z"
    }
   },
   "outputs": [],
   "source": [
    "### Z-Score\n",
    "\n",
    "df_age_fare_outlier = df_age_fare[(z<3).all(axis=1)]\n",
    "\n",
    "print(df_age_fare.shape)\n",
    "print(df_age_fare_outlier.shape)\n",
    "\n",
    "'''\n",
    "It removed about 20 rows from the dataset i.e. outliers have been removed\n",
    "'''\n",
    "\n",
    "\n",
    "### IQR Score\n",
    "\n",
    "df_age_fare_out_iqr = df_age_fare[~((df_age_fare < (Q1 - 1.5 * IQR)) |(df_age_fare > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "df_age_fare_out_iqr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Standardization ( Z-Score)\n",
    "\n",
    "sklearn provides a function called **StandardScaler*. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:19.003400Z",
     "start_time": "2020-06-29T22:04:18.124620Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "df['Age']= std.fit_transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:19.018645Z",
     "start_time": "2020-06-29T22:04:19.005985Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Age\"][1:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minmax\n",
    "\n",
    "sklearn.preprocessinxg.MinMaxScaler(feature_range=(0, 1), copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:19.035729Z",
     "start_time": "2020-06-29T22:04:19.022443Z"
    }
   },
   "outputs": [],
   "source": [
    "##### Min Max normalization\n",
    "\n",
    "# Python provides th min max normalization function in the preprocessing module. \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler(feature_range = (0,1))\n",
    "df['Age']= min_max.fit_transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:04:19.048409Z",
     "start_time": "2020-06-29T22:04:19.038949Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"Age\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Data\n",
    "We rescale each observation to a length of 1 (a unit norm). For this, we use the Normalizer class. Let’s take an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:09:31.807093Z",
     "start_time": "2020-06-29T22:09:31.789644Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normal_scaler = Normalizer()\n",
    "df['Age']= normal_scaler.fit_transform(df[['Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:09:32.917388Z",
     "start_time": "2020-06-29T22:09:32.909044Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical data\n",
    "Sometimes we have categorical varibales in our data. Since the models are based on mathematical equations and caluclations we have to encode the categorical data. Sklearn provides a very efficient tool for encoding the levels of a categorical features into numeric values. There is a class in the library called LabelEncoder which we will use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:09:42.431493Z",
     "start_time": "2020-06-29T22:09:42.424949Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:30.732950Z",
     "start_time": "2020-06-29T22:12:30.714054Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating initial dataframe\n",
    "bridge_types = ('Arch','Beam','Truss','Cantilever','Tied Arch','Suspension','Cable')\n",
    "bridge_df = pd.DataFrame(bridge_types, columns=['Bridge_Types'])\n",
    "# creating instance of labelencoder\n",
    "from sklearn import preprocessing\n",
    "labelencoder = preprocessing.LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "bridge_df['Bridge_Types_Cat'] = labelencoder.fit_transform(bridge_df['Bridge_Types'])\n",
    "bridge_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:30.936726Z",
     "start_time": "2020-06-29T22:12:30.736984Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "\n",
    "df['Embarked']= labelencoder_X.fit_transform(df['Embarked'])\n",
    "df['Embarked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoder\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "\n",
    "sklearn.preprocessing.OneHotEncoder(categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:31.043559Z",
     "start_time": "2020-06-29T22:12:30.940512Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# creating instance of one-hot-encoder\n",
    "onehot_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "\n",
    "onehot_Embarked = pd.DataFrame(onehot_enc.fit_transform(df[['Embarked']]).toarray())\n",
    "#df = pd.concat(df,onehot_Embarked)\n",
    "df = pd.concat([df, onehot_Embarked], axis=1).reindex(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:31.072448Z",
     "start_time": "2020-06-29T22:12:31.047456Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# creating instance of one-hot-encoder\n",
    "onehot_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "\n",
    "onehot_Sex = pd.DataFrame(onehot_enc.fit_transform(df[['Sex']]).toarray(),columns = [\"F\",\"M\"])\n",
    "df = pd.concat([df, onehot_Sex], axis=1).reindex(df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dummies values approach:\n",
    "This approach is more flexible because it allows encoding as many category columns as you would like and choose how to label the columns using a prefix. Proper naming will make the rest of the analysis just a little bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:31.107329Z",
     "start_time": "2020-06-29T22:12:31.076167Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:31.150322Z",
     "start_time": "2020-06-29T22:12:31.111601Z"
    }
   },
   "outputs": [],
   "source": [
    "#dummy_Sex = pd.get_dummies(df, columns = [\"Sex\", \"Embarked\"], prefix = [\"Type_is\"])\n",
    "\n",
    "dummy_Sex = pd.get_dummies(df[[\"Sex\"]])\n",
    "\n",
    "\n",
    "dummy_Embarked = pd.get_dummies(df[[\"Embarked\"]])\n",
    "df = pd.concat([df, dummy_Sex,dummy_Embarked], axis=1).reindex(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization\n",
    "\n",
    "Discretization, also known as quantization or binning, divides a continuous feature into a pre-specified number of categories (bins), and thus makes the data discrete.\n",
    "\n",
    "One of the main goals of a discretization is to significantly reduce the number of discrete intervals of a continuous attribute. Hence, why this transformation can increase the performance of tree based models.\n",
    "\n",
    "*Sklearn* provides a **KBinsDiscretizer**  class that can take care of this. The only thing you have to specify are the number of bins (n_bins) for each feature and how to encode these bins (ordinal, onehot or onehot-dense). The optional strategy parameter can be set to three values:\n",
    "\n",
    "- *uniform*, where all bins in each feature have identical widths.\n",
    "- *quantile* (default), where all bins in each feature have the same number of points.\n",
    "- *kmeans*, where all values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "\n",
    "*Syntax*\n",
    "\n",
    "sklearn.preprocessing.KBinsDiscretizer(n_bins=5, encode='onehot', strategy='quantile')\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:12:31.169390Z",
     "start_time": "2020-06-29T22:12:31.153382Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "#est = KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='uniform')\n",
    "est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "\n",
    "#df['Fare'] = est.fit_transform(df[['Fare']])\n",
    "Fare_disc = est.fit_transform(df[['Fare']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Transformation\n",
    "If you want to convert an existing function into a transformer to assist in data cleaning or processing, you can implement a transformer from an arbitrary function with **FunctionTransformer**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html\n",
    "\n",
    "**syntax**\n",
    "sklearn.preprocessing.FunctionTransformer(func=None, inverse_func=None, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T22:13:15.172432Z",
     "start_time": "2020-06-29T22:13:14.960543Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import math\n",
    "transformer_log = FunctionTransformer(np.log)\n",
    "X = np.array([[10, 20], [30, 40]])\n",
    "print(X,'\\n')\n",
    "print(\"log transformation:\")\n",
    "print(transformer_log.transform(X))\n",
    "\n",
    "\n",
    "# Uisng Pandas apply function.\n",
    "\n",
    "print(pd.DataFrame(X).apply(np.log))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9\n",
    "\n",
    "https://www.simplilearn.com/data-preprocessing-tutorial\n",
    "\n",
    "https://www.deeplearning-academy.com/p/ai-wiki-data-preprocessing\n",
    "\n",
    "https://towardsdatascience.com/data-preprocessing-concepts-fa946d11c825\n",
    "\n",
    "https://data-flair.training/blogs/python-ml-data-preprocessing/\n",
    "\n",
    "https://haridas.in/outlier-removal-clustering.html  # outlier using cluster\n",
    "\n",
    "https://www.pluralsight.com/guides/cleaning-up-data-from-outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
